"""
数据集UCI Repository Adult Dataset
这一数据集包含约32000个训练样本，16000个测试样本。其中有14个特征，包括类别和连续值，其中一些特征缺失。
"""
import pandas as pd
import numpy as np
import catboost as cb
from sklearn.model_selection import KFold
from itertools import product,chain

#1.读取数据
colnames = ['age','wc','fnlwgt','ed','ednum','ms','occ','rel','race','sex','cgain','closs','hpw','nc','label']
train_path = '/Users/dasouche/Downloads/adult.data' 
test_path = '/Users/dasouche/Downloads/adult.test'
train_set = pd.read_csv(train_path, header=None,names=colnames,na_values='?')#将缺失值直接标记为字符串"?"
test_set = pd.read_csv(test_path, header=None,names=colnames,na_values='?',skiprows=[0])
#skiprows需要忽略的行数（从文件开始处算起），或需要跳过的行号列表（从0开始）。

#2.将类别特征列转化为整数
category_cols = ['wc','ed','ms','occ','rel','race','sex','nc','label']
cat_dims = [train_set.columns.get_loc(i) for i in category_cols[:-1]]#查找类别特征列所在位置
for header in category_cols:
    train_set[header] = train_set[header].astype('category').cat.codes
    test_set[header] = test_set[header].astype('category').cat.codes
    
#3.将数据集中的标签分割出来
train_label = train_set['label']
train_set = train_set.drop('label', axis = 1)
test_label = test_set['label']
test_set = test_set.drop('label', axis = 1)

#4.训练默认分类器
clf = cb.CatBoostClassifier()
# cat_dims = [train_set.columns.get_loc(i) for i in category_cols[:-1]] 
clf.fit(train_set, np.ravel(train_label), cat_features=cat_dims)
res = clf.predict(test_set)
print('error:',1-np.mean(res==np.ravel(test_label)))

#5.设置参数范围并使用三折交叉验证

from sortedcontainers import SortedList
import copy
import collections
import numpy as np
from itertools import product,chain
import pandas
from sklearn.model_selection import KFold
import catboost as cb

''' 
a class for doing grid search on a set of parameters provided in a dict. 'pdict' should be a dictionary like the following:
pdict = {'depth':[1,2], 'iterations':[250,100,500], 'thread_count':4}

when grid_search is called it will return an iterator that provides samples from the dictionary e.g.
{'depth':1, 'iterations':250, 'thread_count':4}
{'depth':2, 'iterations':250, 'thread_count':4}
{'depth':1, 'iterations':100, 'thread_count':4}
etc.
after calling an iteration of grid_search, you need to test the classifier and run 'register_result'
This will update the internal list of results, so that the next call to grid_search will use the best
parameters for all the parameters not currently being updated.

grid_search can be provided a list e.g. grid_search(['depth']) this will use the current best parameters for all
the other arguments and only search over 'depth'. You can then call e.g. grid_search(['iterations']) and it will use
the best depth found previously and cycle through all the 'iterations'. Searching incrementally can be much faster
than doing a full grid search, but may miss the global optimum. 
'''

class paramsearch:
    def __init__(self,pdict):    
        self.pdict = {}
        # if something is not passed in as a sequence, make it a sequence with 1 element
        #   don't treat strings as sequences
        for a,b in pdict.items():
            if isinstance(b, collections.Sequence) and not isinstance(b, str): self.pdict[a] = b
            else: self.pdict[a] = [b]
        # our results are a sorted list, so the best score is always the final element
        self.results = SortedList()       
                    
    def grid_search(self,keys=None):
        # do grid search on only the keys listed. If none provided, do all
        if keys==None: keylist = self.pdict.keys()
        else: keylist = keys
 
        listoflists = [] # this will be list of lists of key,value pairs
        for key in keylist: listoflists.append([(key,i) for i in self.pdict[key]])
        for p in product(*listoflists):
            # do any changes to the current best parameter set
            if len(self.results)>0: template = self.results[-1][1]
            else: template = {a:b[0] for a,b in self.pdict.items()}
            # if our updates are the same as current best, don't bother
            if self.equaldict(dict(p),template): continue
            # take the current best and update just the ones to change
            yield self.overwritedict(dict(p),template)
                              
    def equaldict(self,a,b):
        for key in a.keys(): 
            if a[key] != b[key]: return False
        return True            
                              
    def overwritedict(self,new,old):
        old = copy.deepcopy(old)
        for key in new.keys(): old[key] = new[key]
        return old            
    
    # save a (score,params) pair to results. Since 'results' is a sorted list,
    #   the best score is always the final element. A small amount of noise is added
    #   because sorted lists don't like it when two scores are exactly the same    
    def register_result(self,result,params):
        self.results.add((result+np.random.randn()*1e-10,params))    
        
    def bestscore(self):
        return self.results[-1][0]
        
    def bestparam(self):
        return self.results[-1][1]
        

params = {'depth':[3,1,2,6,4,5,7,8,9,10],#树的深度，可以是任何不大于32的整数，推荐1-10
          'iterations':[250,100,500,1000],#最大决策树数目，可以用较小的值
          'learning_rate':[0.03,0.001,0.01,0.1,0.2,0.3], #影响训练的总时长，值越小，训练所需的迭代次数越高
          'l2_leaf_reg':[3,1,5,10,100],#L2正则参数，可以是任意正值
          'border_count':[32,5,10,20,50,100,200],#数值特征分割数，整数1-255（含），
#          'ctr_border_count':[50,5,10,20,100,200],#类别特征分割数，整数1-255（含）；运行中发现该参数存在时报错，去掉正常，故将其注释掉
          'thread_count':4}#训练时采用的cpu核数

# this function does 3-fold crossvalidation with catboostclassifier          
def crossvaltest(params,train_set,train_label,cat_dims,n_splits=3):
    kf = KFold(n_splits=n_splits,shuffle=True) 
    res = []
    for train_index, test_index in kf.split(train_set):
        train = train_set.iloc[train_index,:]
        test = train_set.iloc[test_index,:]
        
        labels = train_label.ix[train_index]
        test_labels = train_label.ix[test_index]
        
        clf = cb.CatBoostClassifier(**params)
        clf.fit(train, np.ravel(labels), cat_features=cat_dims)
        
        res.append(np.mean(clf.predict(test)==np.ravel(test_labels)))
    return np.mean(res)
  
  
  # this function runs grid search on several parameters
def catboost_param_tune(params,train_set,train_label,cat_dims=None,n_splits=3):
    ps = paramsearch(params)
    # search 'border_count', 'l2_leaf_reg' etc. individually 
    #   but 'iterations','learning_rate' together
    for prms in chain(ps.grid_search(['border_count']),
#                       ps.grid_search(['ctr_border_count']),
                      ps.grid_search(['l2_leaf_reg']),
                      ps.grid_search(['iterations','learning_rate']),
                      ps.grid_search(['depth'])):
        res = crossvaltest(prms,train_set,train_label,cat_dims,n_splits)
        # save the crossvalidation result so that future iterations can reuse the best parameters
        ps.register_result(res,prms)
        print(res,prms,'best:',ps.bestscore(),ps.bestparam())
    return ps.bestparam()

bestparams = catboost_param_tune(params,train_set,train_label,cat_dims)#最佳参数

# train classifier with tuned parameters    
clf = cb.CatBoostClassifier(**bestparams)
clf.fit(train_set, np.ravel(train_label), cat_features=cat_dims)
res = clf.predict(test_set)
print('error:',1-np.mean(res==np.ravel(test_label)))#错误率

  



